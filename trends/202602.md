---
period: 2026-02
type: Monthly Report
tags: [Trends, Learning Log]
---

# 🗓️ Intelligence Log: 2026-02

## 🔭 The Observatory (Industry Radar)

> 捕捉行业宏观信号，定义本月技术主旋律。

* **Dominant Theme:** [例如：Reasoning Models & Scaling Law 2.0]
* **The Signal:** [DeepSeek-R1 证明了纯 RL 训练推理模型的可行性，行业焦点从 Pre-train 转向推理侧 Scaling。]

> Use an image to visually summarize these themes and signals. Ensure it has descriptive alt text and supplements the written text.

---

## 🏗️ Technical Review: Models & Directions

> 核心技术点追踪。

### 1. 基座模型追踪 (The Clay)

| 模型名称 | 核心技术创新 (Mechanism) | 关键能力/指标 (Capability) | 业务应用/启发 (Value) |
| :--- | :--- | :--- | :--- |
| [LongCat-Flash-Thinking-2601](https://arxiv.org/abs/2601.16725) | **[Infra] 面向“多环境、多回合、长尾交互”的异步 RL 基建（DORA 扩展）**：<br><br> **多版本+全流式异步流水线**：rollout / env 执行 / reward 计算按 sample 粒度流式推进，支持 trajectory 完成即入队，训练端满足条件即可开训；宣称整体 **2–4×** 快于同步。<br> **超大规模环境并发**：支持最多 **32,000** 环境并发、约 **400** 物理机规模；通过 Lightweight-RolloutManager + RolloutController 分片、扩展 PyTorch RPC 做 CPU idleness-aware 调度。<br> **PD（Prefill–Decode）解耦 + KV-cache CPU swapping**：应对长上下文/多请求下 expert-parallel 组内负载不均与显存紧约束，用异步 chunk 传输 KV 并用 CPU 驻留 KV 规避重算。<br><br> **[Data] “环境可执行/可验证”驱动的 agentic 数据与任务规模化**：<br><br> **Mid-training 结构化 agentic 轨迹补齐稀缺性**：分阶段长上下文激活（32K/128K 阶段 **500B tokens** + 256K 阶段 **40B tokens**），同时注入合成的结构化 agentic 轨迹，为后续大规模 RL 冷启动。<br> **Text-driven synthesis**：从教程/流程文本中做 **workflow 挖掘→工具 schema 提取→多轮对话轨迹合成与精炼**；再做 **Tool decomposition**（把参数隐藏进环境、迫使交互补全）与 **Reasoning decomposition**（每步生成候选、训练决策选择）。<br> **Environment-grounded synthesis**：从工具定义生成轻量 Python 可执行环境；构建 **工具依赖有向图**，采样 tool-chain，**反向合成 prompt**，并通过执行与环境状态校验保证正确性。<br> **Planning-oriented augmentation**：把线性轨迹改造成“分解+早期动作选择” “多候选决策选择”的规划数据。<br> **Cold-start 质量控制**：通用思考数据用 **K-Center-Greedy + sliding-window PPL（512-token window 的 max avg PPL）** 做覆盖+信息量联合选择。tool-use 轨迹用 rubric 校验 + turn-level 质检，并对失败/格式问题 turn 做 **loss masking**。<br><br> **[Train] 统一多域 agentic RL 策略 + 稳定性与鲁棒性共训**：<br><br> **目标函数：GSPO（Group Sequence Policy Optimization）** 做序列级优化，更适合长轨迹；偏 outcome supervision，弱化对长轨迹的惩罚。<br> **两轴 curriculum**（难度/能力要求）+ **动态预算分配**（按实时 pass rate 等指标给 task 估值，近似实现用 pass-rate/历史成功率的 oversampling）。<br> **Self-verification 辅助阶段**：在生成停滞时触发，让模型评估自身 rollout 作为辅助信号加速收敛。<br> **多域环境混训稳定化**：用“域/数据类型分离 oversampling 比例”防止长尾域拖慢异步流水线，同时维持训练混合近似均衡。<br> **显式噪声建模的 Robust RL**：分解真实世界噪声（instruction noise、tool noise），用 curriculum 逐步加噪，但保持任务仍可解。<br> **长程交互的 context management**：summary-based（基于 ReSum，阈值 **80K tokens**，冷启动合成 **15K** summarization 数据）+ discard-all 的混合策略。<br><br> **额外机制**：Zigzag Attention 用 MLA + Streaming Sparse Attention，约 50% 层替换为稀疏层；block size 128、1 sink block + 7 local blocks（每层有效 1,024 token），称可扩展到 **1M tokens** 且端到端约 **1.5×** 推理加速。 | **特点能力 A：Agentic Search / Browsing 强，且对长程交互做了系统性 context 管理**：<br><br> BrowseComp **56.6 / 73.1**（无 / 有 context management，Pass@1）；BrowseComp-ZH **69.0 / 77.7**；RWSearch **79.5**（Pass@1）。<br> 评估前提：BrowseComp 系列区分是否启用 context management；RWSearch 统一不启用。<br><br> **特点能力 B：Agentic Tool-Use 的多域泛化 + 噪声鲁棒**：<br><br> $\tau^{2}$-Bench Avg@4 **88.2**；$\tau^{2}$-Noise Avg@4 **67.1**；VitaBench Avg@4 **29.3**；VitaBench-Noise Avg@4 **20.5**；Random Complex Tasks Avg@4 **35.8**（随机生成复杂任务，主打 OOD 泛化）。<br> 评估前提：噪声集是对同任务多次随机注入噪声后取平均；部分基准在报告中对 simulator/环境问题做了修订以提高可控性。<br><br> **代价/约束**：<br><br> 能力高度依赖“**可执行环境 + 自动验真/rubric + 大规模并发调度**”，复现门槛更像搭 RL 平台而非仅训练模型。<br> Heavy Thinking / 并行思考会显著放大推理成本与延迟（宽度×深度扩展）。<br> 异步多版本训练需要控制 staleness，报告中提到为稳定性会牺牲部分设备利用率。 | **可复用 post-training 路线**：<br><br> **Mid-training 先“教会交互形态”**：在长上下文 staged 激活同时引入中等规模结构化 agentic 轨迹，降低后续 RL 的探索成本。<br> **多域统一 RL**：用 GSPO + 两轴 curriculum +（近似）动态预算分配 + self-verification 辅助阶段，专门面向长轨迹与长尾环境交互。<br> **Robust RL**：把噪声（instruction/tool）显式参数化，按 curriculum 逐层加噪，核心是“不破坏可解性”以避免错误负奖。<br><br> **数据合成与筛选“配方”**：<br><br> **环境缩放（Environment Scaling）**：域规格 → 自动合成工具/DB schema/实现 → 单测 + 调试 agent 校验（报告称 schema→实现成功率 >95%）→ 构建工具依赖图 → 从可执行 tool-chain 出发 BFS 式扩展，只在依赖满足时加节点，确保 executability；再生成“任务描述 + user profile + 评测 rubrics”，并对 rubrics 做一致性校验。<br> **两路 agentic 合成**：Text-driven（挖流程→显式工具调用→轨迹精炼）+ Environment-grounded（执行验证闭环）。<br> * **质量控制细节**：rubric outcome 验真 + turn-level 过滤/format 校验；对低质量 turn 做 loss masking（保留上下文但不让坏动作进梯度）。<br> **通用思考冷启动选数**：KCG + sliding-window PPL（抓局部难点 token）用于“覆盖+信息量”的下采样。<br><br> **落地依赖与改造成本**：<br><br> 必备：可复现沙箱/执行器（代码、工具调用）、统一工具接口、任务判定器/Verifier、评测 harness、轨迹日志回放与失败归因；以及能支撑多环境并发的调度系统。<br><br> **可迁移结论**：<br><br> 优先“**可执行可验证任务集 + rubric/验真**”，再 RL。<br> 做鲁棒性不要只靠线上兜底：把真实噪声类型拆解后“**训练时显式注入 + 课程式加难**”，通常比上线后被动适配更可控。 |

### 2. Post-Training (The Chisel)

| 主题 | 整体趋势/演进 | 核心关注论文 (Key Papers) | 实践参考价值 |
| :--- | :--- | :--- | :--- |

### 3. Agent & Personalization (The Hands)

| 主题 | 整体趋势/演进 | 核心关注论文 (Key Papers) | 实践参考价值 |
| :--- | :--- | :--- | :--- |

---

## 🎨 Fresh Sketches (Monthly Summary)

> 知识摄入复盘，侧重于“判断力”的沉淀而非单纯的数量统计。

* **Crucial Insight:** [本月最大的认知变化是什么？例如：意识到 RLHF 中数据质量远比算法本身重要。]
* **Best Implementation:** [本月发现的最佳开源实现，例如 vLLM 的某个新 Kernel。]

---
