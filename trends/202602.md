---
period: 2026-02
type: Monthly Report
tags: [Trends, Learning Log]
---

# 🗓️ Intelligence Log: 2026-02

## 🔭 The Observatory (Industry Radar)

> 捕捉行业宏观信号，定义本月技术主旋律。

* **Dominant Theme:** [例如：Reasoning Models & Scaling Law 2.0]
* **The Signal:** [DeepSeek-R1 证明了纯 RL 训练推理模型的可行性，行业焦点从 Pre-train 转向推理侧 Scaling。]

> Use an image to visually summarize these themes and signals. Ensure it has descriptive alt text and supplements the written text.

---

## 🏗️ Technical Review: Models & Directions

> 核心技术点追踪。

### 1. 基座模型追踪 (The Clay)

| 模型名称 | 核心技术创新 (Mechanism) | 关键能力/指标 (Capability) | 业务应用/启发 (Value) |
| :--- | :--- | :--- | :--- |
| [LongCat-Flash-Thinking-2601](https://arxiv.org/abs/2601.16725) | **[Infra] 面向“多环境、多回合、长尾交互”的异步 RL 基建（DORA 扩展）**：<br><br> **多版本+全流式异步流水线**：rollout / env 执行 / reward 计算按 sample 粒度流式推进，支持 trajectory 完成即入队，训练端满足条件即可开训；宣称整体 **2–4×** 快于同步。<br> **超大规模环境并发**：支持最多 **32,000** 环境并发、约 **400** 物理机规模；通过 Lightweight-RolloutManager + RolloutController 分片、扩展 PyTorch RPC 做 CPU idleness-aware 调度。![image.png](https://raw.githubusercontent.com/kyriekevin/img-hosting/main/imgs/20260226191942187.png) <br> **PD（Prefill–Decode）解耦 + KV-cache CPU swapping**：应对长上下文/多请求下 expert-parallel 组内负载不均与显存紧约束，用异步 chunk 传输 KV 并用 CPU 驻留 KV 规避重算。![image.png](https://raw.githubusercontent.com/kyriekevin/img-hosting/main/imgs/20260226192014392.png) <br><br> **[Data] “环境可执行/可验证”驱动的 agentic 数据与任务规模化**：<br><br> **Mid-training 结构化 agentic 轨迹补齐稀缺性**：分阶段长上下文激活（32K/128K 阶段 **500B tokens** + 256K 阶段 **40B tokens**），同时注入合成的结构化 agentic 轨迹，为后续大规模 RL 冷启动。<br> **Text-driven synthesis**：从教程/流程文本中做 **workflow 挖掘→工具 schema 提取→多轮对话轨迹合成与精炼**；再做 **Tool decomposition**（把参数隐藏进环境、迫使交互补全）与 **Reasoning decomposition**（每步生成候选、训练决策选择）。<br> **Environment-grounded synthesis**：从工具定义生成轻量 Python 可执行环境；构建 **工具依赖有向图**，采样 tool-chain，**反向合成 prompt**，并通过执行与环境状态校验保证正确性。<br> **Planning-oriented augmentation**：把线性轨迹改造成“分解+早期动作选择” “多候选决策选择”的规划数据。<br> **Cold-start 质量控制**：通用思考数据用 **K-Center-Greedy + sliding-window PPL（512-token window 的 max avg PPL）** 做覆盖+信息量联合选择。tool-use 轨迹用 rubric 校验 + turn-level 质检，并对失败/格式问题 turn 做 **loss masking**。<br><br> **[Train] 统一多域 agentic RL 策略 + 稳定性与鲁棒性共训**：<br><br> **目标函数：GSPO（Group Sequence Policy Optimization）** 做序列级优化，更适合长轨迹；偏 outcome supervision，弱化对长轨迹的惩罚。<br> **两轴 curriculum**（难度/能力要求）+ **动态预算分配**（按实时 pass rate 等指标给 task 估值，近似实现用 pass-rate/历史成功率的 oversampling）。<br> **Self-verification 辅助阶段**：在生成停滞时触发，让模型评估自身 rollout 作为辅助信号加速收敛。<br> **多域环境混训稳定化**：用“域/数据类型分离 oversampling 比例”防止长尾域拖慢异步流水线，同时维持训练混合近似均衡。<br> **显式噪声建模的 Robust RL**：分解真实世界噪声（instruction noise、tool noise），用 curriculum 逐步加噪，但保持任务仍可解。<br> **长程交互的 context management**：summary-based（基于 ReSum，阈值 **80K tokens**，冷启动合成 **15K** summarization 数据）+ discard-all 的混合策略。<br><br> **额外机制**：Zigzag Attention 用 MLA + Streaming Sparse Attention，约 50% 层替换为稀疏层；block size 128、1 sink block + 7 local blocks（每层有效 1,024 token）。![image.png](https://raw.githubusercontent.com/kyriekevin/img-hosting/main/imgs/20260226192412874.png) | **特点能力 A：Agentic Search / Browsing 强，且对长程交互做了系统性 context 管理**：<br><br> BrowseComp **56.6 / 73.1**（无 / 有 context management，Pass@1）；BrowseComp-ZH **69.0 / 77.7**；RWSearch **79.5**（Pass@1）。<br> 评估前提：BrowseComp 系列区分是否启用 context management；RWSearch 统一不启用。<br><br> **特点能力 B：Agentic Tool-Use 的多域泛化 + 噪声鲁棒**：<br><br>![image.png](https://raw.githubusercontent.com/kyriekevin/img-hosting/main/imgs/20260226192126270.png) <br> 评估前提：噪声集是对同任务多次随机注入噪声后取平均；部分基准在报告中对 simulator/环境问题做了修订以提高可控性。<br><br> **代价/约束**：<br><br> 能力高度依赖“**可执行环境 + 自动验真/rubric + 大规模并发调度**”，复现门槛更像搭 RL 平台而非仅训练模型。<br> Heavy Thinking / 并行思考会显著放大推理成本与延迟（宽度×深度扩展）。![image.png](https://raw.githubusercontent.com/kyriekevin/img-hosting/main/imgs/20260226192243764.png) ![image.png](https://raw.githubusercontent.com/kyriekevin/img-hosting/main/imgs/20260226192309761.png) | **可复用 post-training 路线**：<br><br> **Mid-training 先“教会交互形态”**：在长上下文 staged 激活同时引入中等规模结构化 agentic 轨迹，降低后续 RL 的探索成本。<br> **多域统一 RL**：用 GSPO + 两轴 curriculum +（近似）动态预算分配 + self-verification 辅助阶段，专门面向长轨迹与长尾环境交互。<br> **Robust RL**：把噪声（instruction/tool）显式参数化，按 curriculum 逐层加噪，核心是“不破坏可解性”以避免错误负奖。<br><br> **数据合成与筛选“配方”**：<br><br> **环境缩放（Environment Scaling）**：域规格 → 自动合成工具/DB schema/实现 → 单测 + 调试 agent 校验（报告称 schema→实现成功率 >95%）→ 构建工具依赖图 → 从可执行 tool-chain 出发 BFS 式扩展，只在依赖满足时加节点，确保 executability；再生成“任务描述 + user profile + 评测 rubrics”，并对 rubrics 做一致性校验。<br> **两路 agentic 合成**：Text-driven（挖流程→显式工具调用→轨迹精炼）+ Environment-grounded（执行验证闭环）。<br> * **质量控制细节**：rubric outcome 验真 + turn-level 过滤/format 校验；对低质量 turn 做 loss masking（保留上下文但不让坏动作进梯度）。<br> **通用思考冷启动选数**：KCG + sliding-window PPL（抓局部难点 token）用于“覆盖+信息量”的下采样。<br><br> **落地依赖与改造成本**：<br><br> 必备：可复现沙箱/执行器（代码、工具调用）、统一工具接口、任务判定器/Verifier、评测 harness、轨迹日志回放与失败归因；以及能支撑多环境并发的调度系统。<br><br> **可迁移结论**：<br><br> 优先“**可执行可验证任务集 + rubric/验真**”，再 RL。<br> 做鲁棒性不要只靠线上兜底：把真实噪声类型拆解后“**训练时显式注入 + 课程式加难**”，通常比上线后被动适配更可控。 |
| [DeepSeek-OCR 2: Visual Causal Flow](https://arxiv.org/abs/2601.20552) | **[Infra] “视觉 token 先双向、再因果重排”的 LLM-style 编码器**：<br><br> **核心瓶颈**：传统 VLM 将 2D patch 以固定 raster-scan 顺序 + 固定位置编码喂给 LLM（1D 序列建模），对复杂版面引入不合理顺序偏置。<br> **DeepEncoder V2 机制**：把编码器的 CLIP/ViT 部分替换为紧凑 LLM 架构（用 Qwen2-0.5B 作为初始化/骨架），并引入两类 token：<br> **视觉 tokens**：保持 ViT-like **双向注意力**（全局感受野）<br> **causal flow queries**：采用 **因果注意力**，每个 query 可 attend 全部视觉 tokens + 之前的 queries，从而“逐步、语义驱动地重排视觉信息”。 ![image.png](https://raw.githubusercontent.com/kyriekevin/img-hosting/main/imgs/20260226195927683.png) <br> **注意力 mask 关键设计**：块状拼接的双向 + 下三角因果 mask。![image.png](https://raw.githubusercontent.com/kyriekevin/img-hosting/main/imgs/20260226200047250.png) <br> **只输出 queries 给 LLM 解码器**：编码器输出的一半（causal flow tokens）作为压缩后的、带顺序的视觉表示，喂给下游 DeepSeek-MoE decoder。![image.png](https://raw.githubusercontent.com/kyriekevin/img-hosting/main/imgs/20260226200021456.png) <br><br> **[Infra] 高压缩视觉 tokenizer + token budget 约束**：<br> 视觉 tokenizer 沿用 DeepEncoder：**80M 参数 SAM-base + 2 层卷积**，通过 window attention 实现约 **16× token compression**，显著降低后续全局 attention 的算力与激活内存。<br> 视觉 token 控制在 **256–1120**：global view 1024×1024 → 256；local crop 768×768（0–6 个）→ 每个 144，总数 $k \times 144+256\in[256,1120]$。<br><br> **[Data] 数据问题与配比的“小改动大收益”**：<br> 数据源基本沿用 DeepSeek-OCR，**OCR 占训练混合 80%**。<br> 关键改动 1：对 OCR 1.0 按内容分桶更均衡采样（text/formula/table = **3:1:1**）。<br> 关键改动 2：layout 标签精炼（合并语义相近类别，如 figure caption/title）。<br><br> **[Train] 三阶段训练把“重排能力”单独做出来**：<br> Stage-1（encoder pretraining）：语言建模目标（next-token），encoder+轻量 decoder 联训；分辨率 768/1024 两个 dataloader；batch 640，**40k iters**，sequence packing 到 8K，约 **100M image-text pairs**。<br> Stage-2（query enhancement）：冻结视觉 tokenizer，联合优化 LLM-style encoder + LLM decoder；multi-crop 合并数据；**4-stage pipeline parallel**（tokenizer / encoder / DeepSeek-LLM 分两段），global batch 1280，**15k iters**。<br> * Stage-3（decoder specialization）：冻结全部 DeepEncoder V2，仅训 LLM；**20k iters**。<br><br> **TL;DR**：把“2D 文档阅读顺序”显式建模为 **编码器内的因果重排（queries）**，再交给自回归解码器做序列推理，用两级 1D causal 结构逼近“真实 2D 阅读逻辑”。 | **特点能力 A：文档解析总体质量显著提升（高压缩 token 预算下）**：<br> ![image.png](https://raw.githubusercontent.com/kyriekevin/img-hosting/main/imgs/20260226200146115.png) <br><br> **特点能力 B：阅读顺序（reading order）更强，符合“因果视觉流”目标**：<br> ![image.png](https://raw.githubusercontent.com/kyriekevin/img-hosting/main/imgs/20260226200202247.png) <br><br> **特点能力 C：线上可观测质量更稳（重复率下降）**：<br> ![image.png](https://raw.githubusercontent.com/kyriekevin/img-hosting/main/imgs/20260226200214521.png) <br><br> **短板/代价**：<br> **token budget 上限较低时对极密文本可能不利**：论文指出 newspaper 文本 ED 仍可 >0.13，原因包括（1）local crops 数不足；（2）相关训练样本仅约 **250k**，覆盖不足。<br> 该范式的收益强依赖“复杂版面/顺序敏感”任务；对纯自然图像理解未必同等显著。 | **可复用 post-training/训练组织路线**：<br> 把能力拆成 3 段：<br> 先把 encoder 的“压缩 + 重排”学出来（Stage-1）<br> 再做 encoder/decoder 的接口对齐与 query 强化（Stage-2）<br>  最后冻结 encoder、只训 decoder 提吞吐（Stage-3）<br> 当引入“新表征形态”（这里是 reordered queries），**单独安排一个“冻结上游、下游适配”的 continue-train 阶段**，常比端到端硬怼更稳、更省 FLOPs。<br><br> **数据合成与筛选“配方”（文档场景）**：<br> 不是“大换数据”，而是对 OCR 数据做 **内容分桶均衡采样** + **标签体系合并去噪**（layout 类别归并），用更少改动提升泛化。<br> 任何结构化视觉任务都值得先做“按结构类型的 sampling budget”，而不是只追总量。<br><br> **落地依赖与改造成本**：<br> 训练侧：需要较强的分布式能力。<br> 推理/服务侧：需要 multi-crop 视图策略与 visual token 上限控制。<br> 评测侧：配套 OmniDocBench 类的 parsing harness，否则很难定位“顺序建模”是否真起效。<br><br> **可迁移结论**：<br> 当任务天然存在“阅读逻辑/因果顺序”，可以把“顺序”从 prompt/后处理前移到 **encoder 内的可学习重排**，并用“只喂 queries 给 LLM”控制 token 成本。<br> 用 LLM-style encoder + modality-specific queries 的方式，有潜力做 **统一多模态编码器**，从架构上继承 LLM 社区的 MoE/高效 attention 等优化红利。 |

### 2. Post-Training (The Chisel)

| 主题 | 整体趋势/演进 | 核心关注论文 (Key Papers) | 实践参考价值 |
| :--- | :--- | :--- | :--- |

### 3. Agent & Personalization (The Hands)

| 主题 | 整体趋势/演进 | 核心关注论文 (Key Papers) | 实践参考价值 |
| :--- | :--- | :--- | :--- |

---

## 🎨 Fresh Sketches (Monthly Summary)

> 知识摄入复盘，侧重于“判断力”的沉淀而非单纯的数量统计。

* **Crucial Insight:** [本月最大的认知变化是什么？例如：意识到 RLHF 中数据质量远比算法本身重要。]
* **Best Implementation:** [本月发现的最佳开源实现，例如 vLLM 的某个新 Kernel。]

---
