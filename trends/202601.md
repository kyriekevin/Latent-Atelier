---
period: 2026-01
type: Monthly Report
tags: [Trends, Learning Log]
---

# 🗓️ Intelligence Log: 2026-01

## 🔭 The Observatory (Industry Radar)

![Summary of January 2026 industry radar: Dominant themes are On-Policy Distillation, Rubrics/Verifier RL, and Persona Feedback.](https://raw.githubusercontent.com/kyriekevin/img-hosting/main/imgs/20260202143442963.png)

---

## 🏗️ Technical Review: Models & Directions

### 1. 基座模型追踪 (The Clay)

| 模型名称 | 核心技术创新 (Mechanism) | 关键能力/指标 (Capability) | 业务应用/启发 (Value) |
| :--- | :--- | :--- | :--- |
| [Kimi-K2](https://arxiv.org/abs/2507.20534) | **[Infra] 超大 MoE + 长上下文训练形态**：<br> 1.04T MoE（384 experts，token 级激活 8 个，约 32B activated），并在训练后期做 long-context activation（32k→128k，YaRN）。<br><br> **[Data] “token utility”导向的数据增效**：<br> 对知识/数学域做 rephrasing 合成以提升 token utility（在高质量数据受限下的增效式扩容）。<br><br> **[Train] 稳定性突破：MuonClip（Muon + QK-Clip）**：<br> 针对 QK logits/注意力引发的训练不稳定，引入 QK-Clip 稳定训练；宣称 15.5T 预训练全程零 loss spike，并配合分阶段学习率/训练 recipe。![image.png](https://raw.githubusercontent.com/kyriekevin/img-hosting/main/imgs/20260201150520613.png) <br><br> **TL;DR**：把“训练稳定性 + agentic 数据合成 + joint RL”串成一条工程闭环，目标指向开放权重的 agentic 能力上限。 | ![image.png](https://raw.githubusercontent.com/kyriekevin/img-hosting/main/imgs/20260201150259457.png) **特点：agentic / 工具使用 / 软件工程任务强**：<br> 以 Tau2-Bench、ACEBench、SWE-Bench Verified/Multilingual 等作为代表性证据；论文强调 **non-thinking setting** 下仍表现突出（更贴近“默认推理预算”）。<br><br> **特点：多步交互式任务的训练闭环**：<br> 能力来源强调“合成轨迹 + 环境交互 + 联合 RL”，区别于只做静态指令微调。<br><br> **代价/约束**：<br> 优势高度依赖“工具/环境可执行 + 自动判定”的训练与评测形态；没有执行环境时，很多提升路径不可复现或收益折扣。 | **可复用 post-training 路线**：<br> 多阶段后训练 + joint RL；同时覆盖“可验证任务”（可用规则/执行器判定）与“开放式任务”（需要 rubric/judge/critic）。<br><br> **数据合成与筛选配方**：<br> agentic 合成流水线强调：Domain/Tool 生成 → rubric 驱动任务生成 → 多轮轨迹生成（含工具调用与状态更新）→ 质量评估与过滤 → 与真实执行环境混合以“验真 + 纠偏”，等价于把大规模 rejection sampling 工业化。<br><br> **落地依赖与成本**：<br> 需要工具接口标准化、沙箱/执行器（代码、API、检索等）、任务判定器、评测 harness、以及日志回放与失败归因；这些比“换个模型”更像项目本体。<br><br> **迁移结论**：<br> 可迁移的是“可验证环境 + rubric + 过滤 + RL”的闭环，优先从单一业务做可执行任务集与自动判定。 |
| [Minimax-M2](https://www.minimax.io/news/minimax-m2) | **[Infra] “大 MoE、少激活”= 面向 Agent 的单位经济学优化**：230B 总参、每 token 仅激活 10B（稀疏 MoE），核心目标是把 agent 的“plan→act→verify”闭环做得更快、更便宜、更高并发（更小的每请求算力/显存占用）。<br><br> **[Train/Post-train] 以“Agentic RL + 轨迹过滤 + 工程一致性”解决工具环境噪声与不稳定**：MiniMax 披露的 M2 代际经验中，面向多轮工具调用的 RL 会引入外部环境噪声与长尾异常轨迹，因此结合 multiple importance sampling（MIS）与 PPO-style trajectory filtering 来过滤异常统计轨迹以稳定梯度；同时强调训练-推理一致性问题（例如将 prediction layer/LLM head 恢复到 FP32 以改善一致性并解锁持续收益），以及通过日志抽取子轨迹与 prefix merging 提升 RL 训练效率。<br><br> **TL;DR**:用“230B MoE 但仅 10B 激活”的计算形态，把长上下文 + 工具链的 agent 工作流拉到更可部署的成本/延迟区间，并配套以 agentic RL 的稳定训练与轨迹治理思路，推动“能跑起来的工程型 agent”。 | ![image.png](https://raw.githubusercontent.com/kyriekevin/img-hosting/main/imgs/20260201160611254.png) **特点：端到端软件工程/终端闭环能力强**：在官方 repo 披露的端到端 agent/coding 基准中，SWE-bench Verified 69.4、Terminal-Bench 46.3、GAIA(text-only) 75.7、τ²-Bench 77.2 等，覆盖“多文件修改→运行/测试→修复”的闭环任务；评估前提通常是带脚手架与执行环境（终端/工具），更贴近工程生产而非纯文本题库。 <br><br> **特点：通用智能与性价比/速度（可规模化采样）**：Artificial Analysis 给出其 context window 约 205k、输出速度约 96 tok/s、Intelligence Index 分数 36，这类指标对“多次采样/多 seed 回放/大批量轨迹生成”的 agent 训练与评测尤其关键。 <br><br> **代价/约束**： <br> **对“思考标记/历史消息格式”敏感**：其为 interleaved thinking 形态，官方明确要求保留输出中的 \`\` 并原样回传历史，否则性能会受损（这会增加接入与日志/脱敏处理复杂度）。<br> **优势更依赖工具/环境**：多项亮点来自浏览/终端/检索等可执行环境；离开 toolchain 后，收益可能不如“纯推理最强”模型稳定。 | **数据合成与筛选**: 重点不是“再堆 SFT”，而是把外部环境噪声纳入数据治理：对 tool-use 轨迹引入 MIS 纠偏、用 PPO-style trajectory filtering 剔除长尾异常统计轨迹以降低梯度爆炸风险；同时强依赖“自动验真/判定器”（测试、执行返回码、结构化 rubric）来做大规模 rejection sampling。<br><br> **把“激活规模/吞吐”当成 agent 生产力指标**：对 agent 来说，单位时间内可跑的尝试次数（多轮、多 seed、多任务并发）往往比单次极限推理更重要；MoE 小激活 + 高速/低价的组合能直接改变数据生成、评测与迭代节奏。 |
| [LongCat-flash-Omni](https://arxiv.org/abs/2511.00279) | **[Infra] 端到端全模态 MoE + 实时流式架构**：<br> 基于 560B MoE（激活 27B，Shortcut-connected MoE + zero-computation experts），在 Decoder 中集成轻量级音频/视觉编码器与音频解码器（Multi-codebook Audio Detokenization）；设计了“模态解耦并行 (Modality-Decoupled Parallelism)” 训练架构以解决异构数据效率，吞吐量达纯文本训练的 90%。![image.png](https://raw.githubusercontent.com/kyriekevin/img-hosting/main/imgs/20260201160857377.png) <br><br> **[Data] 渐进式多模态 Curriculum + Token Utility 对齐**：<br> 采用 5 阶段渐进式训练（Pre-train → Image/Audio → Video/Spatial → 128K Long Context → Audio Alignment）；数据上不仅包含常规多模态数据，还通过“视觉-语音 QA 转换”与“人机回环 (HITL)”策略构建高质量实时交互数据。![image.png](https://raw.githubusercontent.com/kyriekevin/img-hosting/main/imgs/20260201161009821.png) <br><br> **[Train] 实时交互的流式处理机制**：<br> 引入同步分块交织（Synchronized Chunk-wise Interleaving）策略处理流式输入；在响应生成期间采用稀疏视频采样（0.5 FPS）以平衡视觉保留与计算开销；配合两阶段 SFT（指令跟随 + 交互）与 DPO 强化学习对齐行为。<br><br> **TL;DR**：在超大参数 MoE 基座上，通过“高效流式架构 + 模态解耦并行 + 渐进式全模态对齐”，实现了离线多模态理解与毫秒级实时音视频交互的统一。 | **特点：毫秒级实时音视频交互**：<br> 560B 参数规模下实现毫秒级响应延迟；支持长达 8 分钟以上的连续音视频交互与 128K 上下文长时记忆（Long-term memory）。<br><br> **特点：SOTA 的全模态理解与生成**：<br> Omni-Bench / WorldSense 上取得 SOTA；RealWorldQA (74.8) 性能比肩闭源 Gemini-2.5-Pro；具备 STEM、OCR、GUI Agent 等强单模态能力，且未因多模态融合发生灾难性遗忘。<br><br> **短板/代价**：<br> 视频处理在生成响应期间进行了稀疏采样（0.5 FPS），可能在极高频视觉变化场景下丢失细节；极大的总参数量（560B）对部署显存有硬性门槛，尽管激活参数（27B）较低。 | **可复用 post-training 路线**：<br> 两阶段 SFT（通用指令 + 交互数据）+ DPO 的全模态对齐范式；特别是“视觉-语音 QA 转换”策略，可低成本将离线多模态数据转化为实时交互能力。<br><br> **数据合成与筛选“配方”**：<br> HITL (Human-in-the-Loop) 构建高质量交互数据（注重长时记忆与多轮）；Cross-Modal Data Conversion：将现有的 Vision-Text QA 转换为 Vision-Speech QA，解决高质量语音交互数据稀缺问题。<br><br> **落地依赖与改造成本**：<br> 需要构建支持“分块交织”的流式推理服务架构；训练端需要“模态解耦并行”的基础设施支持以维持高吞吐；数据端强依赖于高质量的音视频对齐与合成管线。 <br><br> **可迁移结论**：<br> “渐进式 Curriculum Training（从单模态到全模态，从短文到长文）”是训练超大模型稳定性的关键；“模态解耦并行”是解决多模态异构数据训练效率的有效工程方案。 |
| [LongCat-flash-Thinking](https://arxiv.org/abs/2509.18883) | ![image.png](https://raw.githubusercontent.com/kyriekevin/img-hosting/main/imgs/20260201161310542.png) **[Infra] DORA 异步强化学习架构**：<br> 引入 Dynamic ORchestration for Asynchronous rollout (DORA)，在大规模分布式 RL 中实现比同步方法快 3 倍的加速；配合 560B MoE (激活 27B) 架构，在推理端引入 **"Heavy Thinking"** 模式（Parallel Thinking + Summary Synthesis）以实现测试时计算扩展 (Test-time Scaling)。<br><br> **[Data] "Cold-Start" 与双路径数据筛选**：<br> 1. **Agentic**：设计“双路径评估” (Dual-path evaluation)，对比有/无工具的轨迹表现，只筛选“必须用工具”的高价值样本，避免工具滥用。<br> 2. **Formal**：利用 Lean 4 证明器进行 Expert Iteration，合成 (Statement, Thinking, Proof) 三元组数据。![image.png](https://raw.githubusercontent.com/kyriekevin/img-hosting/main/imgs/20260201161424389.png) <br><br> **[Train] 域并行 (Domain-Parallel) RL**：<br> 为解决异步 RL 中不同领域 (STEM/Code/Agent) 数据分布差异导致的 negative transfer，采用域解耦优化 + 专家模型融合策略；RL 阶段采用 Token-level loss 和全局常量最大生成长度 (Global constant max gen length) 以缓解长度偏差。 | **特点：极致高效的 Agentic Reasoning**：<br> 在 AIME-25 Benchmark 上，通过高效的工具调用策略，在保持准确率的同时将 Token 消耗降低了 **64.5%** (19k -> 6.9k)；在 Tool-Integrated Reasoning 上表现突出。<br><br> **特点：形式化证明 (Formal Proving) 突破**：<br> 引入 Lean 4 交互环境进行强化训练，在 MiniF2F-Test 上达到 **67.6% (Pass@1)**，显著优于同类开源模型 (通常 <20%)。<br><br> **代价/约束**：<br> 高度依赖大规模异步 RL 框架 (DORA) 的工程稳定性；"Heavy Thinking" 模式虽强但增加了推理延时；形式化推理能力依赖 Lean 4 环境和数据合成管道的构建成本。 | **可复用的 post-training 路线**：<br> Long CoT Cold-Start (Mid-training 课程学习 + Reasoning SFT) $\rightarrow$ Domain-Parallel RL；强调在 RL 前通过 curriculum learning 预热推理能力。<br><br> **数据合成与筛选配方**：<br> **Agentic**：Query 筛选 (Dual-path 判定工具必要性) $\rightarrow$ 多样化轨迹生成 (MCP Servers/Simulated Tools) $\rightarrow$ Model-based Judge 过滤 $\rightarrow$ 标准化。<br> **Math/Formal**：Statement $\rightarrow$ Prover 尝试 $\rightarrow$ Lean 4 验证 $\rightarrow$ Thinking 过程回填合成。<br><br> **迁移结论**：<br> 1. **异步 RL 必须解决域冲突**：混合训练不如“分域训练 + 融合”稳定。<br> 2. **Token 效率是 Agent 核心**：不要只看成功率，要通过 RL 惩罚冗余操作，训练模型“想清楚再行动”以大幅降本。 |
| [DeepSeek-OCR](https://arxiv.org/abs/2510.18234) | **[Infra] "光学上下文压缩" (Context Optical Compression)**：<br> 核心理念是将“长文本”压缩为“高密度视觉 Token”。<br> **Encoder (DeepEncoder)**：结合 Window Attention (SAM 变体，捕捉局部细节) + Global Attention (CLIP 变体，捕捉全局语义) + 2层卷积压缩（16x 下采样），将 1024x1024 图像压缩为仅 ~256 个 token。<br> **Decoder (DeepSeek3B-MoE)**：3B 参数 MoE 架构（共 64 experts，激活 6 个，单次推理仅激活 **570M** 参数），实现轻量化解码。![image.png](https://raw.githubusercontent.com/kyriekevin/img-hosting/main/imgs/20260201161642327.png) <br><br> **[Strategy] 动态分辨率策略 (Gundam Mode)**：<br> 针对密集文档/大图，采用 **"Gundam" 模式**：动态切片（Local Views）+ 全局缩略图（Global View）混合编码，解决固定分辨率下的细节丢失问题。![image.png](https://raw.githubusercontent.com/kyriekevin/img-hosting/main/imgs/20260201161740360.png) <br><br> **[Train] 两阶段训练**：<br> 1. DeepEncoder 独立预训练（对齐视觉与文本语义）；<br> 2. End-to-End 联合训练（冻结部分视觉层，训练 MoE Decoder）。 | **核心指标：极致的 Token 效率 (10x 压缩)**：<br> **OmniDocBench** 评测显示，仅需 **100 visual tokens** 即可达到竞品的精度；<br> 在 10x 压缩率下保持 **97%** 的解码精度，20x 压缩下仍有 ~60% 精度。<br><br> **吞吐量/速度**：<br> 单张 A100 可处理 **200k+ 页/天**。<br> 相比传统 OCR 流水线（检测+识别+组装），端到端延迟显著降低。<br><br> **多模态全能解析**：<br> **结构化输出**：直接输出 Markdown/HTML；<br> **专业领域**：支持手写体、数学公式、化学分子式、几何图形解析；<br> **多语言**：覆盖 100+ 种语言。 | **作为“数据工厂”的价值**：<br> DeepSeek-OCR 的核心定位是 **LLM/VLM 的上游数据清洗引擎**。它能将 PDF/图片转化为高质量的 Markdown 语料，直接用于大模型预训练（Pre-training）。<br><br> **可复用的 Data Pipeline**：<br> **Visual Tokenization**：启示了长文本处理的新路径——用“看”代替“读”。将万字长文压缩为几百个 Visual Tokens 存入 KV Cache，可大幅降低 RAG 或长文档问答的显存占用。<br><br> **落地成本与依赖**：<br> **低算力门槛**：因 MoE 稀疏激活（~570M），消费级显卡即可部署高吞吐服务；<br> **部署依赖**：需配合简单的规则（如裁剪/缩放）作为前处理，无需复杂的传统 OCR 后处理（版面分析/纠错）。<br><br> **方向性启发**：<br> **"Visual is the new Text compression"**：若处理超长 Context，尝试将非结构化数据编码为 Embedding/Visual Token 而非 Raw Text，可能是突破 Context Window 瓶颈的捷径。 |
| [DeepSeek V3.2](https://arxiv.org/abs/2512.02556) | **[Infra] DeepSeek Sparse Attention (DSA)**：<br> 针对长上下文（Long-Context）推理的计算复杂度瓶颈，引入 DSA 替代标准注意力；在保留 V3 MoE (671B/37B active) 高效底座的基础上，大幅降低长序列下的 KV 计算与显存开销，解决“长思考”模式下的效率问题。![image.png](https://raw.githubusercontent.com/kyriekevin/img-hosting/main/imgs/20260201161926459.png) <br><br> **[Train] Scalable RL Protocol (可扩展强化学习协议)**：<br> 提出了一套高稳定性的 RL 训练协议，允许在 Post-training 阶段大幅扩展计算量（Compute Scaling）；支持从“Thinking”模式到“Agentic”任务的跨域对齐，解决了大规模 RL 训练中的发散与 reward hacking 难题。<br><br> **[Data] 专家蒸馏与大规模合成**：<br> 采用 "Specialist Distillation" 策略，先训练 Math/Code/Logic/Agent 六大领域的专用专家模型，再蒸馏回 V3.2 基座；配合合成的 1800+ 虚拟环境与 8.5W+ 复杂 Agent 提示词驱动 RL。<br><br> **TL;DR**：在 V3 极致 MoE 效率基础上，通过 DSA 解决长窗口效率，用“专家蒸馏 + Scalable RL” 突破推理与 Agent 能力上限。 | **特点：金牌级竞赛推理（Speciale 版）**：<br> **DeepSeek-V3.2-Speciale**（放宽长度惩罚的重推理版）在 2025 IMO（数学奥赛）和 IOI（信奥）中均获金牌级表现；Benchmark 上超越 Gemini-3.0-Pro，证明了通过 RL 扩展推理计算量的有效性。<br><br> **特点：通用 Agent 能力**：<br> 在通用 Agent 任务与指令遵循上大幅缩减与闭源模型的差距；核心在于“合成环境”训练，使其不仅能“思考”还能精准调用工具。<br><br> **短板/代价**：<br> 1. **推理模式分化**：提供了 V3.2（通用/效率平衡）与 Speciale（极致推理/高延时）两个版本，暗示单一模型在“极致思考”与“低延迟交互”间仍需由架构/策略进行取舍。<br> 2. **API 复杂度**：引入了特定角色的 prompt 模版（如 `developer` role for search），增加了集成的复杂度。 | **可复用的 post-training 路线**：<br> **Specialist -> Generalist Distillation**：先低成本训练垂类专家（Math/Code），再通过 KL 散度约束等方式蒸馏给通用基座，最后用大规模通用 RL 对齐。这是“能力获取”与“通用性保持”的最优解。<br><br> **数据合成与筛选“配方”**：<br> **Agent 环境合成**：不仅合成 Query，更合成“执行环境”（Sandbox/Mock Environment），通过 1800+ 虚拟环境产生真实的交互轨迹（Trajectory）作为 RL 的 Reward 来源，而非仅靠静态文本评分。<br><br> **落地依赖与改造成本**：<br> 极度依赖**环境仿真能力**。复现其 Agent 能力需要构建大规模的“虚拟执行器”集群来产生训练反馈，而非单纯的文本标注人力。<br><br> **可迁移结论**：<br> 对于长 Context 业务，**Sparse Attention** 是比线性 Attention 更稳健的降本路线；“专家蒸馏”是提升基座特定能力的捷径。 |
| [MiMo-V2-Flash](https://arxiv.org/abs/2601.02780) | **[Infra] 极致效率架构 (Hybrid SWA + MTP)**：<br> 1. **Hybrid Attention**：采用 5:1 的“滑动窗口(SWA, 128) / 全局(GA)”混合比例，KV Cache 压缩约 6 倍，并结合 Learnable Attention Sink 保持长文能力。<br> 2. **Native MTP (Multi-Token Prediction)**：训练时引入轻量级 MTP 模块（Dense FFN, 0.33B），推理时转为“自投机解码”（Self-Speculative Decoding），无需独立 Draft Model。![image.png](https://raw.githubusercontent.com/kyriekevin/img-hosting/main/imgs/20260201162130719.png) <br><br> **[Train] MOPD (Multi-Teacher On-Policy Distillation)**：<br> 后训练阶段引入“多教师在线蒸馏”，利用多个领域特化 Teacher（如数学、代码）提供 Token 级密集奖励（Dense Reward）指导 Student 更新，解决稀疏奖励低效问题。![image.png](https://raw.githubusercontent.com/kyriekevin/img-hosting/main/imgs/20260201162244834.png) <br><br> **[Infra/Train] 稳定性优化**：<br> 引入 **Rollout Routing Replay (R3)** 解决 MoE 在 RL Rollout 与训练时的路由不一致问题；训练全程 FP8 混合精度。<br><br> **TL;DR**：用“混合注意力+MTP”换取极致推理速度，用“多师蒸馏”在有限参数下（15B Active）逼近大模型推理能力。 | **特点：推理速度与显存效率极大化**：<br> 309B 参数仅激活 15B，配合 MTP 实现 **2.6x 解码加速**（Acceptance Length ~3.6）；KV Cache 占用仅为全注意力模型的 1/6，单卡支持更长 Context。<br><br> **特点：代码与 Agent 能力越级挑战**：<br> **SWE-Bench Verified 达到 73.4%**，超越 Kimi-K2 和 DeepSeek-V3.2 Base 等更大模型；LongBench V2 和 NIAH (256k) 表现证明混合注意力未牺牲长文检索精度。<br><br> **短板/代价**：<br> **知识容量受限**：受限于总参数量（309B vs 671B+），在 SimpleQA 等纯知识类任务上弱于顶级全尺寸模型；MTP 带来的加速依赖于任务确定性（低熵任务加速明显）。 | **可复用的 Post-training 路线 (MOPD)**：<br> “领域专家 Teacher → 密集 Reward → 通用 Student”的蒸馏范式，比单纯 SFT 或稀疏 Reward RL 更适合提升特定领域（如代码）上限。<br><br> **落地依赖与架构启发**：<br> **MTP 即 Draft Model**：在预训练阶段集成轻量 MTP 头，推理时直接复用做投机解码，是极低成本的“免费加速”方案。<br> **Hybrid Attention 配方**：SWA (128) + GA (1/6) 是在此量级下平衡“长文显存”与“检索精度”的最优解之一。<br><br> **可迁移结论**：<br> 对于端侧或高吞吐场景，MoE + 混合注意力 + 投机解码是标准答案；RL 阶段必须解决 MoE 路由一致性问题（如使用 R3）。 |
| [GLM-4.7](https://z.ai/blog/glm-4.7) | **[Infra] 深度优先 MoE + MTP + Muon 优化器**：<br> 355B MoE（激活 32B），架构取舍上“重深轻宽”（更少的专家但更多层数）以换取推理性能；引入 MTP (Multi-Token Prediction) 用于投机采样加速；使用 Muon 优化器支持更大 batch size 收敛。<br><br> **[Train] 显式思考策略 (Thinking Policy) 的工程化拆解**：<br> 不仅仅是 CoT，而是将思考分级为：<br> 1. **Interleaved Thinking**：动作/工具调用间的碎片化思考（想-做-想-做）。<br> 2. **Preserved Thinking**：多轮对话中强制保留/回传 Reasoning 内容，解决长程任务中的“遗忘”与上下文不一致。<br> 3. **Turn-level Thinking**：支持按轮次开关思考模式（简单任务关，复杂任务开）。 ![image.png](https://raw.githubusercontent.com/kyriekevin/img-hosting/main/imgs/20260201162446445.png) <br><br> **TL;DR**：用架构深度换推理上限，用“可保留、可开关”的显式思考机制解决 Agent 长程任务的稳定性与成本平衡。 | **特点：Coding Agent 专精与多步容错**：<br> SWE-bench Verified 达 73.8%，Terminal Bench 2.0 达 41%；核心优势在于 **Interleaved Thinking** 下的“工具结果反思能力”，即执行报错后能通过思考自我纠正，而非盲目重试。<br><br> **特点：交互式/前端审美增强**：<br> 针对前端代码生成（HTML/CSS/SVG）做了专门对齐，生成的 UI 布局与配色审美显著提升（Vibe Coding）；在 τ²-Bench (87.4%) 等工具交互榜单上表现突出。<br><br> **代价/约束**：<br> **Preserved Thinking** 需要 API 调用方配合回传 `reasoning_content`，增加了集成的工程复杂度；高强度思考模式下延迟与 Token 消耗显著增加。 | **可复用的 post-training 路线**：<br> **状态保持训练**：在训练数据中构造“多轮”轨迹，强制模型 attend 到上一轮的 reasoning content，使其学会“接力思考”而非每次重起炉灶。<br><br> **落地依赖与成本**：<br> **端侧配合**：要发挥 Preserved Thinking 威力，Client 端必须维护 Reasoning 缓存并回传（类似 KV Cache 的应用层变体）；<br> **成本控制**：利用 Turn-level Thinking 接口，在业务层编写规则（Router），对查询分类后动态决定是否开启思考模式，实现“智障/天才”按需切换。<br><br> **迁移结论**：<br> 不要把 Reasoning 仅当作生成的“副产品”，而应将其视为**可缓存、可复用、可路由**的“中间状态资产”。 |

### 2. Post-Training (The Chisel)

| 主题 | 整体趋势/演进 | 核心关注论文 (Key Papers) | 实践参考价值 |
| :--- | :--- | :--- | :--- |
| 训练范式 | 1. **交互演进**：<br> 从固定数据集的“白盒离线模仿” -> “在线交互生成与蒸馏”演进。 <br> 2. **数据演进**：<br> 从依赖人类标注 -> 模型自生合成数据 -> 带纠错的强化数据。 <br> 3. **范式迁移**：<br> 从“结果驱动”的性能指标 -> “过程驱动”的可靠智能。 | [On policy Distillation](https://thinkingmachines.ai/blog/on-policy-distillation/) <br> **场景/问题**：<br> 针对 SFT Off-Policy 训练存在分布偏移以及 RL 训练反馈稀疏、算力成本高的问题 <br> **方法思路**：<br> 用教师模型对学生自生成路径进行 Token 级别打分，通过 Reverse KL 强制模型进行 Mode-seeking 学习，而非平均化的概率拟合。 <br><br> [GAD Black-Box On-Policy Distillation](https://arxiv.org/abs/2511.10643) <br> **场景/问题**：<br> 针对 SOTA 模型闭源、无法获取 Logit 概率分布，导致传统蒸馏只能进行浅层文本克隆的问题。 <br> **方法思路**：<br> 通过构建一个类似 GAN 的系统，使用协同进化的判别器作为动态奖励模型，在黑盒环境下将蒸馏转化为判别器与生成器博弈，迫使学生模型通过强化学习捕捉教师的深层逻辑特征。 <br><br> [DeepSeekMath-V2](https://arxiv.org/abs/2511.22570) <br> **场景/问题**：<br> 针对复杂推理中“答案正常但步骤错误”的伪智能问题，以及模型在 OOD 难题上逻辑崩溃问题。 <br> **方法思路**：<br> 通过生成器-验证器-元验证器的三模型协同训练，让模型在生成过程中进行自我审计于多次修正。 | 1. **高效轻量化注入流程**：<br> 采用 `Student Rollout -> Teacher Feedback -> Student Refine` 的迭代流程，在将大模型指令能力注入小模型同时有效缓解“灾难性遗忘”。 <br> 2. **开放式问题的自我进化**：<br> 在无标准答案场景下，引入 GAD 模式，判别器与生成器协同进化，防止模型陷入 Reward Hacking，实现业务数据的持续闭环自进化。 <br> 3. **可靠性工程**：<br> 对于容错率低的业务场景，落地“生成-验证-修正”的三元 Agent 架构，利用元验证器对过程进行强干预，实现从“概率性输出”到“确定性推理”的工程化。 |
| RL Reward | 1. **奖励白盒化**：<br> 从依赖不透明的黑盒偏好模型（Black-box RM） -> 基于显式规则（Rubrics）和检索证据的可解释奖励。 <br> 2. **粒度结构化**：<br> 从单一的标量打分 -> 多维度的 Checklist 评分，将主观评价转化为可执行的原子标准。 <br> 3. **约束严苛化**：<br> 从允许“部分正确”的连续奖励 -> 零容忍的“二元奖励（Binary）”，强迫模型进行高精度的自我校准。 | [Rubrics as Rewards](https://arxiv.org/abs/2507.17746) <br> **场景/问题**：<br> 针对开放式任务缺乏标准答案，且传统 RLHF 奖励信号模糊、易导致 Reward Hacking 的问题。 <br> **方法思路**：<br> 将模糊的“好坏”评价拆解为结构化的 Checklist（评分细则），包含基本事实、逻辑步骤及负面避坑点（Pitfalls），为模型提供细粒度、可解释的监督信号。 <br><br> [Binary Retrieval-Augmented Reward Mitigates Hallucinations](https://arxiv.org/abs/2510.17733) <br> **场景/问题**：<br> 针对 RAG 场景下模型容易产生“似是而非”的幻觉，且传统连续值奖励容易导致模型为了高分而牺牲通用能力的问题。 <br> **方法思路**：<br> 引入极其严苛的二元奖励机制（Binary RAR），结合检索系统验证，只有当输出完全符合事实时才给分（1），否则为 0，拒绝“部分给分”的妥协。 | 1. **主观任务的“客观化”训练**：<br> 利用 RaR 将开放式问题拆解为可执行的 Checklist，使小模型也能作为可靠裁判监督大模型，显著降低对昂贵人工或超大模型的依赖。 <br> 2. **通用能力无损的幻觉消除**：<br> Binary RAR 证明了在事实性训练中，“一票否决”的严苛奖励能有效训练出“知之为知之”的诚实模型（Calibrated Abstention），在大幅降低幻觉的同时不发生“对齐税”（即不损伤代码、数学等通用能力）。 <br> 3. **可解释的防作弊机制**：<br> 通过离散化、基于规则的奖励设计，让优化目标透明化，显著减少了模型通过“钻空子”讨好判别器而非提升质量来获取高分的现象。 |

### 3. Agent & Personalization (The Hands)

| 主题 | 整体趋势/演进 | 核心关注论文 (Key Papers) | 实践参考价值 |
| :--- | :--- | :--- | :--- |
| Agent Memory | 1. **存储范式**：<br> 从“静态、被动检索”的 RAG/MemoryBank -> “动态、自我进化”的图谱化记忆。 <br> 2. **检索机制**：<br> 从简单的“语义相似度匹配” -> “Deep Research 深度研究”式的多跳推理检索。 <br> 3. **多智能体协同**：<br> 从“单点记忆” -> “多智能体层级化共享记忆”，支持跨试错（Cross-trial）知识复用。 | [General Agentic Memory via Deep Research](https://arxiv.org/abs/2511.18423) <br> **场景/问题**：<br> 传统静态记忆（预编译摘要）存在严重信息丢失，且难以应对复杂多跳推理任务。 <br> **方法思路**：<br> 提出 **Just-in-Time (JIT) 编译**范式。 <br> **Memorizer（离线）**：仅构建轻量级索引，保留全量历史数据（Page Store），避免过早压缩导致的信息丢失。 <br> **Researcher（在线）**：利用思维链（CoT）进行迭代式规划、搜索和反思（Reflection），针对当前问题动态生成最优上下文。<br><br> [G-Memory](https://arxiv.org/abs/2506.07398) <br> **场景/问题**：<br> 多智能体系统（MAS）缺乏有效的记忆共享机制，导致协作经验无法沉淀，跨任务迁移能力差。 <br> **方法思路**：<br> 借鉴组织记忆理论，构建**三层图谱结构（Insight / Query / Interaction Graphs）**。 <br> **双向检索**：向上获取高层通用洞察（Insight），向下获取细粒度交互轨迹（Trajectory）。 <br> **动态演进**：通过图结构的持续更新，实现团队协作经验的自我进化和跨 Agent 复用。 ![image.png](https://raw.githubusercontent.com/kyriekevin/img-hosting/main/imgs/202601312142144.png) ![image.png](https://raw.githubusercontent.com/kyriekevin/img-hosting/main/imgs/202601312143252.png) ![image.png](https://raw.githubusercontent.com/kyriekevin/img-hosting/main/imgs/202601312144287.png)<br><br> [A-Mem: Agentic Memory for LLM Agents](https://arxiv.org/html/2502.12110v1) <br> **场景/问题**：<br> 现有记忆系统结构僵化，无法像人类一样进行联想记忆和知识重组，难以应对开放式任务。 <br> **方法思路**：<br> 借鉴 **卢曼卡片盒笔记法 (Zettelkasten)**。 <br> **自动链接 (Link Generation)**：基于属性和上下文描述，自动建立新旧记忆间的语义连接。 <br> **记忆进化 (Memory Evolution)**：新经验触发旧记忆的重写和更新，涌现出更高阶的模式和概念。 | 1. **无损长程记忆架构**：<br> 放弃过度压缩的摘要式记忆，转向“全量存储 + 按需深挖”的 JIT 模式。利用“Memorizer + Researcher”双智体架构，显著提升复杂任务（如 NarrativeQA, HotpotQA）的召回精度。 <br> 2. **团队协作经验沉淀**：<br> 在多智能体（MAS）场景下，部署分层图记忆系统（如 G-Memory），将显性的交互日志转化为隐性的协作策略（Insight），提升团队在 embodied action 等任务上的成功率（最高提升 20%）。 <br> 3. **知识图谱的动态生长**：<br> 引入类似 A-Mem 的自组织机制，让 Agent 像人类一样通过“记笔记”和“建立连接”来管理记忆，实现从“数据堆砌”到“知识网络”的质变，适用于需要长期学习和推理的 Personal Assistant 场景。 |
| Agent 架构 | 1. **组织形态**：<br> 从“静态预定义拓扑” -> “RL 驱动的动态编排与演化”。 <br> 2. **设计哲学**：<br> 从盲目堆叠复杂度的“性能优先” -> 关注 Cost-of-Pass 的“效能成本均衡”。 <br> 3. **能力集成**：<br> 从单体 Prompt 拼接 -> “即插即用”的标准化技能模块 (Modular Skills)。 | [Multi-Agent Collaboration via Evolving Orchestration](https://arxiv.org/abs/2505.19591) <br> **场景/问题**：<br> 现有 Multi-Agent 系统多依赖静态固定的组织结构（如固定流水线），难以适应动态任务复杂度，且随着 Agent 数量增加导致协作开销剧增。 <br> **方法思路**：<br> 提出“Puppeteer（提线木偶）”范式。引入一个中心化编排器（Orchestrator），利用强化学习（RL）根据任务状态动态选择和排序 Agent（Puppets）。系统会在训练中自发涌现出“紧凑化”和“循环式”的高效推理结构，自动剪枝低效路径。<br><br> [Efficient Agents: Building Effective Agents While Reducing Cost](https://arxiv.org/abs/2508.02694) <br> **场景/问题**：<br> SOTA Agent 系统成本极其高昂（Token 消耗大），且简单的堆叠模块（如无限增加 Planning 步数）往往面临收益递减，缺乏对“效能-成本”权衡的系统性研究。 <br> **方法思路**：<br> 提出 `Cost-of-Pass` 指标，量化分析 Backbone、规划模块、工具使用对成本与性能的影响。<br><br>[Anthropic Skills](https://github.com/anthropics/skills) <br> **场景/问题**：<br> Agent 能力通常硬编码在 Prompt 中，难以复用且上下文占用高；缺乏一套通用的、跨环境的 Agent 能力定义标准。 <br> **方法思路**：<br> 定义了一套标准化的“Skills”目录结构（包含 `SKILL.md` 指令与资源）。采用 `Browse -> Load -> Use` 的三阶段动态加载机制，仅在需要时注入特定能力，实现 Agent 能力的模块化管理和低上下文开销。 | 1. **动态路由架构**：<br> 在复杂业务流中放弃固定 DAG 图，转向部署“中心化路由模型（Router/Orchestrator）”，利用业务反馈数据训练路由策略，让系统自动发现最优协作路径。 <br> 2. **ROI 驱动的工程优化**：<br> 建立 Agent 系统的 `Cost-of-Pass` 监控面板。对于 Planning 步数、Best-of-N 采样数等超参，必须基于“单位成本带来的性能增益”进行剪裁，拒绝盲目的复杂度扩张。 <br> 3. **能力解耦与标准化**：<br> 采用类似 Anthropic Skills 的标准将业务逻辑封装为独立模块。实施“能力懒加载”策略，仅将工具描述（而非完整 Prompt）放入初始上下文，大幅降低首字延迟与 Token 成本。 |
| Agent 通信与共识机制 | 1. **交互多样性演进**：<br> 从同质化思维的“群体盲思” -> 异质化视角的“对抗性辩论”演进。 <br> 2. **决策协议演进**：<br> 从单一的“多数投票” -> 任务适配的“共识vs投票”分流策略。 <br> 3. **效率与独立性**：<br> 从冗余的多轮共识 -> 强调独立起草 (AAD) 与反从众 (Anti-Conformity) 的高效机制。 | [Multi-Agent Debate Strategies to Enhance Requirements Engineering with Large Language Models](https://arxiv.org/abs/2507.05981) <br> **场景/问题**：<br> 针对 RE (需求工程) 任务中单 Agent 仅依赖单次输出，缺乏迭代和协作导致鲁棒性差的问题。 <br> **方法思路**：<br> 引入 MAD (Multi-Agent Debate) 策略分类学，通过模拟人类辩论的多视角特性，构建多 Agent 协作框架，在需求分类等任务中通过辩论减少偏差，提升分类准确性。<br><br> [Breaking Mental Set to Improve Reasoning through Diverse Multi-Agent Debate](https://openreview.net/forum?id=t6QHYUOQL7) <br> **场景/问题**：<br> 针对多 Agent 即使分配不同角色，仍倾向于使用同质化推理路径，陷入“思维定势 (Fixed Mental Set)”的问题。 <br> **方法思路**：<br> 提出 DMAD (Diverse Multi-Agent Debate)，强制不同 Agent 采用截然不同的推理策略/视角进行辩论。通过异质化思维碰撞，打破同质化共识的陷阱，提升复杂推理任务的上限。 <br><br> [Voting or Consensus? Decision-Making in Multi-Agent Debate](https://arxiv.org/abs/2502.19130) <br> **场景/问题**：<br> 针对当前 MAD 系统不加区分地使用投票或共识协议，忽略任务特性导致性能受限的问题。 <br> **方法思路**：<br> 系统性验证了“投票机制”更适合推理任务 (提升13.2%)，“共识机制”更适合知识任务 (提升2.8%)。提出了 AAD (All-Agents Drafting) 和 CI (Collective Improvement) 机制，强制 Agent 在辩论前独立起草方案，防止早期趋同（Sycophancy）。 | 1. **任务适配的决策路由**：<br> 在实际应用中建立路由机制：对于逻辑推理类任务，采用“投票+独立起草”策略以探索多路径；对于知识问答/事实类任务，采用“多轮共识”策略以利用互查纠错。 <br> 2. **抗趋同设计**：<br> 为防止 Agent 间互相“抄作业”导致的虚假一致性，工程上应强制实施“独立思考阶段” (如 Hidden Scratchpad)，只有在生成独立草稿后才允许进行 Agent 间通信。 <br> 3. **异质性专家网络**：<br> 不仅仅是分配不同 Prompt 角色（如律师/医生），更要在系统层面集成思维方式完全不同的 Agent，通过 DMAD 模式提升复杂场景下的决策质量。 |
| Agent 自我演进与进化 | 1. **工具创造演进**：<br> 从“被动调用人工定义的 API” -> “主动基于代码库/文档封装新工具”。<br> 2. **技能沉淀演进**：<br> 从“隐式参数记忆/Context 记忆” -> “显式、可执行的代码化技能库 (Skill Library)”。<br> 3. **学习机制演进**：<br> 从“依赖提示词的 In-Context Learning” -> “基于强化学习 (RL) 的全生命周期自我进化”。 | [LLM Agents Making Agent Tools](https://arxiv.org/abs/2502.11705) <br> **场景/问题**：<br> 针对科学计算、医学等长尾领域，人工开发 Agent 工具成本高、覆盖率低的问题。<br> **方法思路**：<br> 提出 ⁠ToolMaker 框架，赋予 Agent “工具制造”能力。输入 GitHub URL，Agent 自主完成“环境依赖安装 -> 功能代码提取与封装 -> 单元测试与闭环 Debug”，将静态代码库转化为可调用的 Agent Tools。<br><br> [SE-Agent](https://arxiv.org/abs/2508.02085) <br> **场景/问题**：<br> 针对复杂多步推理任务中，Agent 容易陷入局部最优或无法从历史轨迹中吸取教训的问题。<br> **方法思路**：<br> 提出一种基于“自我进化轨迹优化”的方法。通过 修正 (Revision)、重组 (Recombination) 和 提炼 (Refinement) 三个阶段，利用跨轨迹的灵感来扩展搜索空间，实现推理路径的自我迭代与优化。<br><br> [Reinforcement Learning for Self-Improving Agent with Skill Library](https://arxiv.org/abs/2512.17102) <br> **场景/问题**：<br> 针对 Agent 在新环境中无法持续积累经验，且传统 Prompt 方式调用技能库效率低、准确率差的问题。<br> **方法思路**：<br> 提出 ⁠SAGE 框架 (Skill Augmented GRPO for self-Evolution)。利用 Sequential Rollout (串行任务链) 训练 Agent 在解决任务过程中将高频操作封装为可执行的 Python 函数 (Skill)，并通过 技能集成奖励 强化模型复用这些技能，实现“越用越强”。 | 1. **“工具工厂”模式落地**：<br> 为垂类 Agent 落地提供了低成本方案——通过自动化流水线将企业现有的代码资产 (Legacy Code/Scripts) 直接转化为 Agent 可调用的工具层，无需人工重写。<br> 2. **降本增效的技能库**：<br> SAGE 证明了显式技能库 (Skill Library) 具有极高的工程价值。通过将复杂推理固化为代码函数复用，不仅提升了任务成功率，更大幅降低了 Token 消耗。<br> 3. **闭环进化架构**：<br> 无论是制造工具还是优化推理，核心在于构建“执行 -> 验证/Debug -> 修正 -> 沉淀”的闭环。未来的 Agent 系统设计应包含“经验提取器”，将运行时的成功路径转化为永久性的资产 (工具或技能)。 |
| 个性化建模 | 1. **数据源演进**：<br> 从“隐式推理”（对话历史推测） -> “显式定义 + 动态修正”（Persona定义+交互反馈）。 <br> 2. **反馈机制**：<br> 从“人工标注排序/打分” -> “用户自然交互信号”（点赞/Love/Rewrites）。 <br> 3. **模型形态**：<br> 从“通用模型+Prompt” -> “双塔结构/独立适配器” -> “全链路个性化Agent”。 | [The Era of Real-World Human Interaction: RL from User Conversations](https://arxiv.org/abs/2509.25137) <br> **场景/问题**：<br> 现有模型依赖专家标注的静态数据，无法适应真实用户多样化、长周期的偏好，且无法从自然交互（如追问、纠错）中持续学习。 <br> **方法思路**：<br> 提出 RLHI (Reinforcement Learning from Human Interaction)，包含：1) **User-Guided Rewrites**: 利用用户追问/纠错的自然语言反馈，作为负反馈信号指导模型修正；2) **User-Based Rewards**: 构建基于用户长期历史（Persona）的奖励模型，进行个性化偏好优化。<br><br> [Reinforcement Learning from User Feedback](https://arxiv.org/abs/2505.14946) <br> **场景/问题**：<br> 专家标注与真实用户偏好不一致，且真实反馈稀疏、非结构化（如点赞/Emoji）。 <br> **方法思路**：<br> 提出 RLUF，利用轻量级用户反馈（如“Love Reaction”）训练奖励模型 $P[\text{Love}]$，并将其纳入多目标强化学习框架（Helpfulness + Safety + Love），在保证安全与能力前提下最大化用户喜爱度。<br><br> [PersonaFeedback](https://arxiv.org/abs/2506.12915) <br> **场景/问题**：<br> 缺乏高质量的个性化评估基准，现有基准多依赖隐式推理，难以区分“推理能力”与“个性化生成能力”。 <br> **方法思路**：<br> 构建“显式Persona + Query”的评估数据集，解耦Persona推理与生成任务，提供Easy/Medium/Hard分级测试，特别是针对微小风格差异的 Hard 样本进行细粒度评估。 <br><br> [PersonaAgent](https://arxiv.org/abs/2506.06254) <br> **场景/问题**：<br> 通用Agent缺乏灵活性，无法动态适应用户偏好变化；现有方法多为“一刀切”或静态Prompt。 <br> **方法思路**：<br> 提出全链路个性化Agent架构：1) **个性化记忆模块**（情景+语义记忆）；2) **动态Persona**：作为中间层连接记忆与动作，实时演进；3) **Test-Time Alignment**：在推理时通过模拟交互优化Persona Prompt，实时对齐用户偏好。 <br><br> [Querier-Aware LLM](https://arxiv.org/abs/2412.11736v2) <br> **场景/问题**：<br> 现有个性化多关注“扮演者(Responder)”的性格，忽略了“提问者(Querier)”的背景差异（如专家 vs 小白对同一问题的不同需求）。 <br> **方法思路**：<br> 采用双塔架构（通用编码器 + 提问者特定低秩编码器），通过查询相似度聚类和对比学习（Querier-Contrastive Loss），拉近同一提问者的风格表征，拉开不同提问者的差异，实现“千人千面”的回答。 <br><br> [GEM](https://www.arxiv.org/abs/2511.13007) <br> **场景/问题**：<br> 个性化偏好数据稀缺（Few-shot），且往往包含复杂的认知推理偏好，传统奖励模型难以捕捉。 <br> **方法思路**：<br> 提出生成式熵导向偏好建模。利用熵作为不确定性指标筛选高价值思维链（CoT），并通过自生成的偏好对进行对比学习，在低资源下捕捉深层认知偏好。 | 1. **低成本反馈闭环**：<br> 落地 RLUF 机制，利用业务中已有的点赞、收藏、追问等“废气数据”作为奖励信号，替代昂贵的标注数据，实现模型在线持续进化。 <br> 2. **动态与静态结合的工程架构**：<br> 参考 PersonaAgent，构建“长期语义记忆 + 动态Prompt优化”的架构，既保留用户长期画像，又能通过 Test-time Optimization 适应即时需求。 <br> 3. **差异化服务能力**：<br> 借鉴 Querier-Aware 思想，在POI问答等领域，针对不同认知水平的用户（如儿童 vs 成人）动态调整回复的深度与语气，而非仅调整“人设”。 |

---

## 🎨 Fresh Sketches (Monthly Summary)

* **Crucial Insight:** 本月最大的认知变化是：**Post-train 才是“可复利的主战场”**，而且正在收敛成一套可工程化的“三件套”：  
    1) **On-Policy Distillation**（用学生自生成分布+教师/裁判的密集反馈，持续注入能力并减轻分布偏移）；  
    2) **Rubrics / Verifier RL**（把奖励白盒化、结构化甚至二值化，让开放式任务也能稳定优化且更抗 reward hacking）；  
    3) **Persona Feedback / User Interaction RL**（把对齐从“平均人类偏好”推进到“个体长期偏好”，让产品数据天然变成训练信号）。  
    结果是：竞争优势不再主要来自“更大模型/更强算法”，而来自**闭环系统资产**（rubric 库、判定器、轨迹治理、反馈渠道与评测 harness）。
* **Best Implementation:** **Clawbot / OpenClaw（Clawbot 系）**：把“Agent”做成了一个可落地的工程形态——它更像一个**本地运行的 agent gateway**（不绑定单一模型），长期常驻、可接入多种消息渠道与本机工具/文件/脚本，并用 **skills + workspace(Markdown/文件夹)** 的方式把能力沉淀为可复用模块；同时提供 CLI 入口便于自动化与集成。这套实现之所以“爆火”，本质是它把 *tool-use、记忆、权限与渠道* 这些最难产品化的部分，直接给了一个可用的开源参考答案（也顺带提醒了“高权限常驻代理”的安全边界问题）。

---
